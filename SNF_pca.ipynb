{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNF with PCA to remove s100b batch effect\n",
    "s100b is a biological covariate that causes noise in clustering.\\\n",
    "PCA is applied to each dataset. Principal components that are most correlated to s100b are removed. SNF is run on remaining principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy\n",
    "import scipy.stats as stats\n",
    "\n",
    "import snf\n",
    "from snf import metrics\n",
    "from sklearn.cluster import spectral_clustering\n",
    "from sklearn.metrics import v_measure_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data():\n",
    "    # Proteomics\n",
    "    prot = pd.read_csv('/nfs/answer/fraenkel_internal/current_omics/proteomics/normalized_imp_corrected_proteomics204_03292022.csv')\n",
    "    prot = prot.set_index('Unnamed: 0')\n",
    "    \n",
    "    # Transcriptomics\n",
    "    tran = pd.read_csv('/nfs/answer/fraenkel_internal/current_omics/transcriptomics/rna_norm_196_02222023.csv.gz')\n",
    "    tran = tran.set_index('Unnamed: 0')\n",
    "\n",
    "    # Epigenomics\n",
    "    epig = pd.read_csv('/nfs/answer/fraenkel_internal/current_omics/epigenomics/atac_norm_196_02222023.csv.gz')\n",
    "    epig = epig.set_index('Unnamed: 0')\n",
    "    \n",
    "    # Metadata\n",
    "    meta = pd.read_csv('/nfs/answer/fraenkel_internal/current_omics/proteomics/full_prot_metadata203_03082023.csv',\n",
    "                        error_bad_lines=False)\n",
    "    meta = meta.set_index('Unnamed: 0')\n",
    "    \n",
    "    return [prot, tran, epig, meta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch rows and columns to match snf format\n",
    "def transpose(data):\n",
    "    data_return = []\n",
    "    for d in data[:-1]:\n",
    "        data_return.append(d.T)\n",
    "    data_return.append(data[-1])\n",
    "    \n",
    "    return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only samples present in all datasets\n",
    "def filter_and_sort(data):\n",
    "    \n",
    "    meta = data[-1]\n",
    "    epig = data[-2]\n",
    "    \n",
    "    meta = meta[meta.index.isin(epig.index)]\n",
    "\n",
    "    data_return = []\n",
    "    \n",
    "    for d in data:\n",
    "        d = d[d.index.isin(meta.index)]\n",
    "        data_return.append(d.sort_index())\n",
    "    \n",
    "    return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns without s100b\n",
    "def remove_s100b_na(data):\n",
    "    meta = data[-1]\n",
    "    meta = meta[~meta['s100b'].isna()]\n",
    "    data_return = []\n",
    "    \n",
    "    for d in data[:-1]:\n",
    "        d = d[d.index.isin(meta.index)]\n",
    "        data_return.append(d)\n",
    "    data_return.append(meta)\n",
    "    \n",
    "    return data_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "data = transpose(data)\n",
    "data = filter_and_sort(data)\n",
    "prot, tran, epig, meta = remove_s100b_na(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA is applied to each dataset\n",
    "def do_pca(data):\n",
    "    pca = PCA(n_components=50, random_state=5)\n",
    "    meta = data[-1]\n",
    "    s100b = meta['s100b']\n",
    "    \n",
    "    pca_data = []\n",
    "    scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "    \n",
    "    for df in data[:-1]:\n",
    "\n",
    "        df = pd.DataFrame(scaler.fit_transform(df), columns = df.columns, index = df.index)\n",
    "        \n",
    "        keep = df.var().sort_values(ascending=False)\n",
    "\n",
    "        df = df[keep.index[:round(len(keep)/2)]]\n",
    "        principalComponents = pca.fit_transform(df)\n",
    "        principalDf = pd.DataFrame(data = principalComponents, index = df.index)\n",
    "        \n",
    "        # Principal components that are most correlated to s100b are removed\n",
    "        drop = []\n",
    "        for col, colData in principalDf.iteritems():\n",
    "            r = spearmanr(s100b, colData)[0]\n",
    "            if r > 0.3 or r < -0.3:\n",
    "                drop.append(col)\n",
    "                \n",
    "        pca_data.append(principalDf.drop(columns=drop))\n",
    "    \n",
    "    pca_data.append(meta)\n",
    "    \n",
    "    # Remaining principal components are returned\n",
    "    return pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_pca, tran_pca, epig_pca, meta = do_pca([prot, tran, epig, meta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case\n",
    "actual_labels = meta['Case'].map({'CASE': 1, 'CTRL': 0}).array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply SNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_snf(prot, tran, epig):\n",
    "    '''\n",
    "    Apply SNF to data.\n",
    "    Args:\n",
    "        prot: DataFrame of normalized proteomics data\n",
    "        tran: DataFrame of normalized transcriptomics data\n",
    "        epig: DataFrame of normalized epigenomics data\n",
    "    Returns:\n",
    "        best: best number of clusters by spectral clustering\n",
    "        affinity_networks: array with coefficients of similarity\n",
    "            between each pair of patients for a single modality\n",
    "        fused_network: array with coefficients of fused similarity\n",
    "            between each pair of patients\n",
    "        fused_labels: array of label of 0/1 for each patient,\n",
    "            determined by SNF and spectral clustering\n",
    "    '''\n",
    "    data = [prot, tran, epig]\n",
    "    \n",
    "    # K: num of nearest neighbors to consider when constructing affinity matrix\n",
    "        # good value for K is sqrt(N)\n",
    "    # mu: scaling factor that weights affinity matrix\n",
    "    affinity_networks = snf.make_affinity(data, metric='euclidean', K=14, mu=0.5)\n",
    "    \n",
    "    # Run SNF algorithm\n",
    "    fused_network = snf.snf(affinity_networks, K=14)\n",
    "    \n",
    "    # Estimate the number of clusters in the data via the “eigengap” method\n",
    "    best, second = snf.get_n_clusters(fused_network)\n",
    "    \n",
    "    # Get labels from clusters\n",
    "    fused_labels = spectral_clustering(fused_network, n_clusters=best)\n",
    "    \n",
    "    return best, affinity_networks, fused_network, fused_labels\n",
    "\n",
    "# With proteomics data normalized by code\n",
    "#best, fused_network, fused_labels = apply_snf(prot_z, tran_z, epig_z)\n",
    "# With already normalized proteomics data\n",
    "best, affinity_networks, fused_network, fused_labels = apply_snf(prot_pca,\n",
    "                                                                 tran_pca,\n",
    "                                                                 epig_pca)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v_measure_score': 0.03134589907277121,\n",
       " 'nmi': array([[1.        , 0.0313459 , 0.00294143, 0.00492028, 0.02259586],\n",
       "        [0.0313459 , 1.        , 0.01125853, 0.0662375 , 0.21249449],\n",
       "        [0.00294143, 0.01125853, 1.        , 0.00127733, 0.0029285 ],\n",
       "        [0.00492028, 0.0662375 , 0.00127733, 1.        , 0.03545742],\n",
       "        [0.02259586, 0.21249449, 0.0029285 , 0.03545742, 1.        ]]),\n",
       " 'silhouette score': 0.037547142466343754}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scores to evaluate performance of SNF\n",
    "def score(best, affinity_networks, fused_network, fused_labels, actual_labels):\n",
    "    '''\n",
    "    Scores to evaluate performance of SNF\n",
    "    Returns:\n",
    "        scores_dict: dictionary of key=score name, value=score value\n",
    "            for v_measure_score, nmi, and silhouette score\n",
    "    '''\n",
    "    # v_measure_score ranges from 0 to 1, where 1 indicates perfect overlap between the derived and true labels and 0 indicates no overlap\n",
    "    v_measure = v_measure_score(fused_labels, actual_labels)\n",
    "    \n",
    "    # Generate cluster labels from the individual affinity matrices\n",
    "    labels = [actual_labels, fused_labels]\n",
    "    for arr in affinity_networks:\n",
    "        labels += [spectral_clustering(arr, n_clusters=best)]\n",
    "        \n",
    "    '''\n",
    "    Normalized mutual information score (NMI) between the labels\n",
    "    generated by SNF and the ones we just obtained.\n",
    "    0 indicates no overlap and 1 indicates a perfect correspondence\n",
    "    between the two sets of labels.\n",
    "    '''\n",
    "    nmi = normalized_mutual_info_score(actual_labels, fused_labels)\n",
    "    '''\n",
    "    Silhouette score\n",
    "    Range from -1 to 1, where -1 indicates a poor clustering solution\n",
    "    and 1 indicates a fantastic solution.\n",
    "    '''\n",
    "    np.fill_diagonal(fused_network, 0)\n",
    "    sil = metrics.silhouette_score(fused_network, fused_labels)\n",
    "    \n",
    "    return v_measure, nmi, sil\n",
    "\n",
    "scores = score(best, affinity_networks, fused_network, fused_labels, actual_labels)\n",
    "scores_dict = {'v_measure_score': scores[0], 'nmi': scores[1], 'silhouette score': scores[2]}\n",
    "scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network visualization\n",
    "Visualize connections within/between clusters generated by SNF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "from pyvis import network as net\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node_colors(network, col_attribute, cmap_name=\"Accent\"):\n",
    "    \"\"\"\n",
    "    Function to add node colors based on existing node attribute.\n",
    "    \n",
    "    network: networkx object\n",
    "    col_attribute: Name of attribute to use for determining node colors\n",
    "    cmap_name: Name of matplotlib colormap to use (default: 'Accent')\n",
    "    \"\"\"\n",
    "    source_att = nx.get_node_attributes(network, col_attribute)\n",
    "    \n",
    "    # Get colors\n",
    "    cmap = cm.get_cmap(cmap_name, len(set(source_att.values())))    \n",
    "    source_dict = dict(zip(list(set(source_att.values())),\n",
    "                           np.arange(len(set(source_att.values())))))\n",
    "    node_colors = {n:mpl.colors.rgb2hex(cmap(source_dict[v]), keep_alpha=True) for (n, v) in source_att.items()}\n",
    "    # Set colors\n",
    "    nx.set_node_attributes(network, node_colors, 'color')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.00676185, 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00564674],\n",
       "       ...,\n",
       "       [0.00676185, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00568299],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.00564674, ..., 0.00568299, 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(fused_network)\n",
    "fused_network_vis = fused_network.copy()\n",
    "\n",
    "# Set all entries below 75% quartile to 0 to avoid too many network connections\n",
    "cutoff = np.percentile(fused_network_vis, 75)\n",
    "for row in fused_network_vis:\n",
    "    row[row < cutoff] = 0\n",
    "fused_network_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Case': 'CASE', 'Sex.x': 'Female', 'fused_labels': 0, 'color': '#7fc97fff'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create networkx object\n",
    "nt = nx.from_numpy_matrix(fused_network_vis)\n",
    "nt = nx.relabel_nodes(nt, dict(zip(np.arange(n), prot.index.values)))\n",
    "\n",
    "# Set some attributes from metadata\n",
    "meta_net = meta.copy()\n",
    "meta_net['fused_labels'] = fused_labels\n",
    "nx.set_node_attributes(nt, meta_net.loc[:, ['Case', 'Sex.x', 'fused_labels', ]].to_dict('index'))\n",
    "\n",
    "# Set node colors \n",
    "add_node_colors(nt, col_attribute='fused_labels')\n",
    "nt.nodes['NEUAB000NKC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "nx.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"750px\"\n",
       "            height=\"750px\"\n",
       "            src=\"nx.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fd4b46ab550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subgraph for ease of visualization\n",
    "sub_nt = nt.subgraph(list(nt.nodes)[:75])\n",
    "\n",
    "ntw = net.Network('750px', '750px', notebook=True)\n",
    "ntw.toggle_physics(False)\n",
    "ntw.from_nx(sub_nt)\n",
    "\n",
    "ntw.show('nx.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fresh-env)",
   "language": "python",
   "name": "fresh-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
